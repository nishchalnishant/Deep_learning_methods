{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "5.2 K-Nearest Neighbors for Regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdLP6qa3gdpn"
      },
      "source": [
        "# 5.2 K-Nearest Neighbors for Regression\n",
        "\n",
        "Linear regression is not the only machine learning model that Ashenfelter could have fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwOMv33Jgdpp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
        "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
        "                          index_col=\"year\")\n",
        "\n",
        "# Split the data into training and test sets.\n",
        "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
        "bordeaux_test = bordeaux_df.loc[1981:].copy()\n",
        "\n",
        "# Log transform the target.\n",
        "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])\n",
        "bordeaux_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obIRRRtTgdpu"
      },
      "source": [
        "Let's focus on just two features for now: winter rainfall (**win**) and average summer temperature (**summer**). Let's plot the training data, using a color gradient to represent the target (**log(price)**). Notice how we can customize the color gradient using the `cmap=` argument. A list of the available colormaps can be found [here](https://matplotlib.org/examples/color/colormaps_reference.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LpV-ttBgdpw"
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "bordeaux_train.plot.scatter(x=\"win\", y=\"summer\", c=\"log(price)\", \n",
        "                            cmap=cm.YlOrRd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr_2sHvrgdpz"
      },
      "source": [
        "Notice how wines that are close on this scatterplot are of similar quality. This insight is the basis of the $k$-nearest neighbors algorithm for predicting wine quality. Suppose that we want to predict the quality of the 1986 vintage, represented by a blue star in the plot below. \n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/regression_neighbors.png?raw=1)\n",
        "\n",
        "The $k=5$ points that are closest to this point in feature space are indicated by dotted lines. We can average the qualities of thse wines to obtain our prediction. All 5 of these points have a quality less than 3.0, so the 1986 vintage is also predicted to be of low quality.\n",
        "\n",
        "The $k$-nearest neighbors regression algorithm can be summarized as follows:\n",
        "\n",
        "1. Determine the $k$ closest points in the training data to the new point that you want to predict for, based on some distance metric on the features.\n",
        "2. The predicted label of the new point is the mean (or median) of the labels of the $k$ closest points.\n",
        "\n",
        "Let's implement this in code. First, we extract the training data and scale the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LMYQFf7gdp0"
      },
      "source": [
        "X_train = bordeaux_train[[\"win\", \"summer\"]]\n",
        "y_train = bordeaux_train[\"log(price)\"]\n",
        "\n",
        "# Standardize the features.\n",
        "X_train_mean = X_train.mean()\n",
        "X_train_sd = X_train.std()\n",
        "X_train_st = (X_train - X_train_mean) / X_train_sd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp1jBJqNgdp4"
      },
      "source": [
        "Now, we get the features for the new observation (i.e., the 1986 vintage), standardizing it in the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt0eaZ3Cgdp6"
      },
      "source": [
        "x_new = bordeaux_test.loc[1986, [\"win\", \"summer\"]]\n",
        "\n",
        "x_new_st = (x_new - X_train_mean) / X_train_sd\n",
        "x_new_st"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRheuLl-gdp-"
      },
      "source": [
        "Now we calculate the (Euclidean) distances between the 1986 vintage and the vintages in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJK7pzwmgdp_"
      },
      "source": [
        "dists = np.sqrt(((X_train_st - x_new_st) ** 2).sum(axis=1))\n",
        "dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOn2bnnZgdqC"
      },
      "source": [
        "Now, we sort the distances. The first 5 of these are the nearest neighbors. To get the year of these nearest neighbors, we get the index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHCsio0NgdqE"
      },
      "source": [
        "i_nearest = dists.sort_values().index[:5]\n",
        "i_nearest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvLMrn-hgdqG"
      },
      "source": [
        "We can look up these years in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2GRYEEsgdqH"
      },
      "source": [
        "bordeaux_train.loc[i_nearest]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzpFu5_GgdqK"
      },
      "source": [
        "To make a prediction for the price of the 1986 vintage, we average the sale prices of these 5-nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfnayFCmgdqM"
      },
      "source": [
        "y_train.loc[i_nearest].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2dNakSEgdqP"
      },
      "source": [
        "So the model predicts that the quality of the 1986 is about 2.56, which is well below the average quality.\n",
        "\n",
        "Of course, the model above only had two features so it was easy to visualize the \"nearest neighbors\" on the scatterplot. The magic of $k$-nearest neighbors is that it still works when there are more features and the data is not possible to visualize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDVZHOz8gdqQ"
      },
      "source": [
        "## K-Nearest Neighbors in scikit-learn\n",
        "\n",
        "Now let's see how to implement $k$-nearest neighbors in scikit-learn. Recall from the previous lesson that all scikit-learn models follow the three-step pattern:\n",
        "\n",
        "1. Declare the model.\n",
        "2. Fit the model to training data.\n",
        "3. Use the model to predict on test data.\n",
        "\n",
        "To fit a $k$-nearest neighbors model instead of a linear regression model, we only need to modify the first step. Instead of declaring a model of type `LinearRegression`, we define a model of type `KNeighborsRegressor`, specifying the value of $k$ as one of the parameters. Because `KNeighborsRegressor` works with distances, it is a good idea to scale the features before passing the features into the model. (Refer back to Chapter 3 for a full explanation.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z8RozqFgdqQ"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Standardize the training and test data\n",
        "scaler = StandardScaler()\n",
        "X_train_st = scaler.fit_transform(X_train)\n",
        "X_new_st = scaler.transform(pd.DataFrame([x_new])) # needs to be a DataFrame\n",
        "y_train = bordeaux_train[\"log(price)\"]\n",
        "\n",
        "# Fit k-nearest neighbors\n",
        "model = KNeighborsRegressor(n_neighbors=5)\n",
        "model.fit(X=X_train_st, y=y_train)\n",
        "model.predict(X=X_new_st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otJtjyw1gdqT"
      },
      "source": [
        "This is the same predicted value that we got by implementing $k$-nearest neighbors manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNrdiBTaggrM"
      },
      "source": [
        "In the code above, we had to be careful to standardize the training data and the test data in exactly the same way before fitting the $k$-nearest neighbors model. Most machine learning models have many more preprocessing steps. As the preprocessing gets more complex, it is easy to accidentally omit one of the preprocessing steps. For this reason, scikit-learn provides a _Pipeline_ object, which simply chains together a sequence of preprocessing and model building steps. If we call `Pipeline.fit()` or `Pipeline.predict()` on the data, all of the steps will be applied to the data in a consistent manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSFA_1MFggjH"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        ")\n",
        "\n",
        "pipeline.fit(X=X_train, y=y_train)\n",
        "pipeline.predict(X=pd.DataFrame([x_new]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUqRnbagdqU"
      },
      "source": [
        "## The K-Nearest Neighbors Regression Function\n",
        "\n",
        "A predictive model is simply a function $f$ that maps feature values ${\\bf x}$ to target values $y$. We can visualize $f$ when ${\\bf x}$ consists of just a single feature, such as **age**. In the previous lesson, we saw that $f$ is just a line when the model is linear regression. What does $f$ look like when the model is a $k$-nearest neighbors regressor?\n",
        "\n",
        "First, we extract the training data. There is no need to scale the features in this case because there is only one feature. (The point of scaling is to bring all of the variables to the same scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLegUnjjgdqV"
      },
      "source": [
        "X_train = bordeaux_train[[\"age\"]]\n",
        "y_train = bordeaux_train[\"log(price)\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seSY9rIhgdqY"
      },
      "source": [
        "Now we fit the $k$-nearest neighbor model as before. We do not need to standardize the feature in this case because there is only one feature. Standardizing is only useful when there are multiple features that we want to bring to the same scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7yrXhKgdqZ"
      },
      "source": [
        "# Fit k-nearest neighbors\n",
        "model = KNeighborsRegressor(n_neighbors=5)\n",
        "model.fit(X=X_train, y=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8S3Mvpgdqe"
      },
      "source": [
        "In order to graph $f$, we need to evaluate the predictive model at a grid of feature values. Since age ranges from 12 to 40 in the training data, we create a grid of ${\\bf x}$ values from 10 to 45, make predictions at these values, and plot these predictions as a curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odZD51z6gdqf"
      },
      "source": [
        "# Define a grid of feature values.\n",
        "X_new = pd.DataFrame()\n",
        "X_new[\"age\"] = np.linspace(10, 45, num=200)\n",
        "\n",
        "# Make predictions at those feature values.\n",
        "y_new_ = pd.Series(\n",
        "    model.predict(X_new),\n",
        "    index=X_new[\"age\"]\n",
        ")\n",
        "\n",
        "# Plot the predictions.\n",
        "bordeaux_train.plot.scatter(x=\"age\", y=\"log(price)\")\n",
        "y_new_.plot.line()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMkIpaWSgdqi"
      },
      "source": [
        "Unlike the linear regression model, the $k$-nearest neighbor model is piecewise constant. For example, wines more than 37 years old all have the same 5-nearest neighbors, so the prediction is constant in that range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY5hYuQQgdqj"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ac0WvSvgdqk"
      },
      "source": [
        "1\\. Plot the $k$-nearest neighbors regression function for predicting the quality of a wine from its age for $k=1, 5, 10, 20$. How does the regression function change as $k$ increases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWTD3k7Ugdql"
      },
      "source": [
        "2\\. Using the Ames housing data set (http://dlsun.github.io/pods/data/AmesHousing.txt ), fit a $k$-nearest neighbors model that predicts the price of a home using square footage, number of bedrooms (**Bedroom AbvGr**), number of full bathrooms (**Full Bath**), and number of half bathrooms (**Half Bath**). Then, use your fitted model to predict the price of a home that is 1500 square feet, with 3 bedrooms, 2 full baths, and 1 half bath."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyjjnFAgdql"
      },
      "source": [
        "3\\. You would like to predict how much a male diner will tip on a bill of \\\\$40.00 on a Sunday. Build a $k$-nearest neighbors model to answer this question, using the tips data (http://dlsun.github.io/pods/data/tips.csv ) as your training data."
      ]
    }
  ]
}