{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classification algorithm is based off of **Bayes’ Theorem**. It is a **supervised** classification algorithm which can be used for **both categorical and numerical feature data**. \n",
    "\n",
    "**Bayes’ Theorem** uses **conditional probabilities** to “predict” the outcome of later probabilities. The Bayesian concept of a **priori probability** is important: it is used as a ground truth from which we can use to obtain the outcome of other probabilities. Let’s take a look at Bayes’ theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://uc-r.github.io/public/images/analytics/naive_bayes/naive_bayes_icon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(c) indicates the a priori probability of the given class, while P(x) represents the probability of a given predictor. Priori probability is always known a priori from the training examples,posteriori probability is the probability of the class (or model) which we calculate after seeing the data (or evidence or events). Likelihood answers the question that “What is the probability of the data (or evidence or events) that it has been generated from the model (or class or outcome) ?” Please refer to this [blog](https://appliedmachinelearning.blog/2017/05/23/understanding-naive-bayes-classifier-from-scratch-python-code/) for detailed explanation with example.\n",
    "\n",
    "**Why called Naive??**\n",
    "\n",
    "The model is very naively believing that the effect of the occurrence of any of the events is completely independent of the occurrence of other events. This is obviously not the case with most of the real world problems and the predictor features are known to influence and be influenced by other features. Still, naive Bayes is known to provide results at par and sometimes even better than highly complex and computationally expensive classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Naive Baye's from scratch for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the formula, we need to calculate class prior probabilities **P(c)**, Likelihood **P(x/c)** and predictor prior probabilities **P(x)** to implement the algorithm and calculate the posterior probability for any given test data. Note that we **donot need** to calculate predictor prior probability **P(x)** as it will remain **same for all posterior probability calculation** for all possible output classes. Since we will be using class with maximum posterior probability as the prediction class, P(X) just **acts as a scaling factor** and can be ignored.\n",
    "\n",
    "**P(c)** is calculated as:\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*-PkUQ4n42T1YLaK-jXI9VQ.png)\n",
    "\n",
    "i.e divide occurrence of given class by sum of all class occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood **P(x/c)** is calculated as: \n",
    "\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*PB2b5dB2rHffvx_g-wcSng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e for a given feature level wi  we count how many of wi belong in class c. We then divide this by ALL the w that belong to c. This gives us a probability for a wi given c. We can make a minor adjustment to this formula to smooth the data (this accounts for the possibility that some feature level wi could have a zero-count for a class, which would make eliminate our work done so far). Please refer to the blog above for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have above to information, given a test data point, we can calculate the posterior probability **P(c/x)** and select the highest probability class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Creating Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LabeledTextData:\n",
    "    id_num: int\n",
    "    label: str\n",
    "    tokens: list\n",
    "\n",
    "train = [LabeledTextData(42, 'cat',  \"🐈 🐯 🐱 🐩 🐱\".split()),\n",
    "         LabeledTextData(43, 'dog',  \"🐶 🐶 🐈 🐶 🐩 🐈 🐶 🐶\".split()),\n",
    "         LabeledTextData(45, 'cat',  \"🐈 🐈 🐯 🐶 🐈\".split()),\n",
    "         LabeledTextData(45, 'cat',  \"🐈 🐈 🐈\".split()),\n",
    "         LabeledTextData(48, 'dog',  \"🐶 🐶 🐯 🐈 🐩 🐱 🐩 🐶 🐩 🐶 \".split()),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data class is a way for creating a new data type with custom attributes. Learn more about dataclasses [here](https://realpython.com/python-data-classes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating class priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_priors(labels):\n",
    "    doc_priors = defaultdict(float)\n",
    "\n",
    "    for label in labels:\n",
    "        doc_priors[label] = sum(1 for d in labels if d == label) / len(labels)\n",
    "\n",
    "    return doc_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 0.6, 'dog': 0.4})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [d.label for d in train]\n",
    "prior = doc_priors(labels)\n",
    "prior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculating Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we do following things. We first create a dictionary of various unique features. In our case these are the shapes. If we had a word document, it would have been unique words and the collection of unique words would have been our vocabulary. Once we have created a vocabulary, we calculate the likelihood for each output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(train, labels):\n",
    "    \n",
    "    # A default dict of default dicts; inner default dict is probability\n",
    "    cond_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for label in set(labels):\n",
    "    \n",
    "        label_tokens = []\n",
    "        for i, doc in enumerate(train):\n",
    "             # For a given label, get a list of all the tokens for all the docs \n",
    "            if labels[i] == label:\n",
    "                label_tokens.extend(doc)\n",
    "\n",
    "        for token in set(label_tokens):\n",
    "            # Find conditional probability: token count / total count\n",
    "            cond_prob[label][token] = label_tokens.count(token) / len(label_tokens) \n",
    "\n",
    "    return cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.likelihood.<locals>.<lambda>()>,\n",
       "            {'cat': defaultdict(float,\n",
       "                         {'🐈': 0.5384615384615384,\n",
       "                          '🐩': 0.07692307692307693,\n",
       "                          '🐯': 0.15384615384615385,\n",
       "                          '🐶': 0.07692307692307693,\n",
       "                          '🐱': 0.15384615384615385}),\n",
       "             'dog': defaultdict(float,\n",
       "                         {'🐈': 0.16666666666666666,\n",
       "                          '🐩': 0.2222222222222222,\n",
       "                          '🐯': 0.05555555555555555,\n",
       "                          '🐶': 0.5,\n",
       "                          '🐱': 0.05555555555555555})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "train_x =[x.tokens for x in train]\n",
    "cond_prob = likelihood(train_x, labels)\n",
    "cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat', 'dog'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_prob.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculating Posterior Probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def post_prob(test, cond_prob, doc_priors):\n",
    "    prob_predicted = defaultdict(float)\n",
    "    for label in cond_prob.keys():\n",
    "        # For each label, calculate the conditional probability based on the prior and the tokens that appear\n",
    "        prob_predicted[label] = doc_priors[label] * product(cond_prob[label][t] for t in test)\n",
    "    \n",
    "    return prob_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 0.09230769230769231, 'dog': 0.022222222222222223})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = LabeledTextData(id_num=90, label=None, tokens=\"🐱\".split())\n",
    "# test = LabeledTextData(id_num=91, label=None, tokens=\"🐶 🐶\".split()) \n",
    "# test = LabeledTextData(id_num=92, label=None, tokens=\"🐶 🐱\".split())\n",
    "# test = LabeledTextData(id_num=93, label=None, tokens=\"🐈 🐈 🐶 🐶 🐩 🐱 🐱\".split())\n",
    "# test = LabeledTextData(id_num=94, label=None, tokens=\"🐬 \".split()) # Out of sample prediction\n",
    "test_x = test.tokens\n",
    "prob_predicted = post_prob(test_x, cond_prob, prior)\n",
    "prob_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: cat\n"
     ]
    }
   ],
   "source": [
    "label, prob = max(prob_predicted.items(),\n",
    "                  key=itemgetter(1))\n",
    "print(\"The predicted class is:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together in a class\n",
    "We will make a class so that the it becomes easier to apply the algorithm and make predictions. We will also make slight changes by using **log soft probability calculation** so the **model performs much better** even for large number of features and we **donot have the issue of multiplying with 0 if a feature is missing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes_MN:\n",
    "    def __init__(self):\n",
    "        self.voccab = []\n",
    "        self.labelset = None\n",
    "        self.n_items = None\n",
    "        self.priors = defaultdict(float)\n",
    "        self.prob_predicted = defaultdict(float)\n",
    "        self.c_word_dict = defaultdict(lambda: defaultdict(float))\n",
    "        self.class_count = defaultdict(float)\n",
    "    \n",
    "    def fit(self, x_train, labels):\n",
    "        self.x_train = np.array(x_train)\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_items = self.labels.size\n",
    "        self.labelset = set(self.labels)\n",
    "        \n",
    "        word_dict = defaultdict(list)\n",
    "        for i,doc in enumerate(x_train):\n",
    "            self.voccab.extend(doc)\n",
    "            word_dict[labels[i]].extend(doc)\n",
    "        \n",
    "        for i in self.labelset:\n",
    "            self.c_word_dict[i] = Counter(word_dict[i])\n",
    "            self.class_count[i] = np.sum(list(self.c_word_dict[i].values())) \n",
    "        \n",
    "        self.voccab = set(self.voccab)\n",
    "        self.voccab_len = len(self.voccab)\n",
    "        self.doc_priors()\n",
    "\n",
    "    \n",
    "    def doc_priors(self):\n",
    "        for label in self.labelset:\n",
    "            self.priors[label] = np.sum(1 for d in self.labels if d == label)*1.0 / self.n_items\n",
    "    \n",
    "    def likelihood(self, word, label):\n",
    "        return np.log((self.c_word_dict[label][word]+1.0)/(self.class_count[label]+self.voccab_len + 1.0))\n",
    "\n",
    "    def post_prob(self, test):\n",
    "        for label in self.labelset:\n",
    "            self.prob_predicted[label] = np.log(self.priors[label]) \n",
    "            for t in test:\n",
    "                self.prob_predicted[label] += self.likelihood(t, label)\n",
    "        return self.prob_predicted\n",
    "\n",
    "    def predict(self, test):\n",
    "        self.test_labels = []\n",
    "        for i in test:\n",
    "            prob_predicted = self.post_prob(i)\n",
    "            label, prob = max(prob_predicted.items(),\n",
    "                      key=itemgetter(1))\n",
    "            self.test_labels.append(label)\n",
    "        return self.test_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on previous test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes_MN()\n",
    "clf.fit(train_x, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'cat': 13, 'dog': 18})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🐱']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([test_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on Real data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sklearn's 20newsgroups data set and consider only 4 categories to test our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med'] \n",
    "newsgroups_train=fetch_20newsgroups(subset='train',categories=categories)\n",
    "\n",
    "train_data=newsgroups_train.data #getting all trainign examples\n",
    "train_labels=newsgroups_train.target #getting training labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to tokenize the data and get it in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # returning the preprocessed string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [preprocess_string(x).split() for x in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes_MN()\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test=fetch_20newsgroups(subset='test',categories=categories) #loading test data\n",
    "test_data=newsgroups_test.data #get test set examples\n",
    "test_labels=newsgroups_test.target #get test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [preprocess_string(x).split() for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy is 0.939\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted accuracy is %.3f\" %(np.sum(test_labels == predicted)/len(predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have build a good enough model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive Bayes (NB) classifier is a probabilistic classifier that assumes complete independence between the predictor features. They perform quite well even though the working behind them is simplistic. There exist other methods as well to determine the likelihoods of the features. Thus the NB classifiers are actually a family of classifiers rather being just a single classifier. Below are the classifiers available in any machine learning library, such as sklearn for Python.\n",
    "\n",
    "**Classification based on feature type**\n",
    "\n",
    "**Gaussian NB** – The likelihood of the features is assumed to be Gaussian distribution. We will implement **[Gaussian NB in a separate notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Gaussian_Naive_Bayes.ipynb)**. \n",
    "\n",
    "**Multinomial NB** – This works for multinomially distributed data like word count vector where features in the vectors are probability of the event appearing in the sample. The algorithm remains the same. We just pre-calculate the word counts per document and feed it and rest of the steps are same. Interestingly tf-idf values also work in even though they are continuous numbers and the base algorithm does not change.\n",
    "\n",
    "**Bernoulli NB** – This is used for multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (boolean) variable. The only change is in the way conditional probability is calculated and can be easily implemented by just changing the likelihood function. Check [this notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Gaussian_Naive_Bayes.ipynb) for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Naive Bayes\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "2. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "3. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation which we used above.\n",
    "2. On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "3. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips to improve the power of Naive Bayes Model\n",
    "\n",
    "\n",
    "1. If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
    "2. Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n",
    "3. Naive Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True|False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your  pre-processing of data and the feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
