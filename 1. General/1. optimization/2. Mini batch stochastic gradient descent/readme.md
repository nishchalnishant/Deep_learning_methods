**Pros**

- Convergence is more stable than Stochastic Gradient Descent
  Computationally efficient
- Fast Learning since we perform more updates.

**Cons**

- We have to configure the Mini-Batch size hyperparameter
