> Covers general topics covering deep learning

# optimization

- Adam optimizers
- Scholastic gradient discent
- Rms prop
- Adam
- Scholastic gradient discent with momentum

# Skip Connection blocks

- Residual block
- Bottleneck residual block
- Dense block
- Inverted residual block

# Attention

- Additive attension
- Multi head attention
- Scaled dot product attention

# Stochastic optimization

- Sgd
- Sgd with momentum
- Ada Grad
- Ada delta
- Rms prop

# Regularization

- Weight decay
- Label smoothing
- L1 regularization
- Early stopping
- Dropout
- Entropy regularization
- Attention dropout

# Activation function

- Most of the activation function belongs to the similar families i.e. has some differences between them

- ReLU
- Sigmoid activation
- Tanh
- Softplus
- Leaky ReLU
- P ReLU
- Swish

# Normalization

- Batch Normalization
- Layer normalization
- Instance normalization
- Spectral normalization
- Group normalization

# Loss functions

- Ctc loss
- Gan least squares loss
- Cycle consistency loss
- Focal loss
- Triplet loss

# Learning rate schedules

- Linear warm up with linear decay
- Linear warmup with cosine annealing
- Stop decay
- Exponential decay

# Feed forward networks

- Dense connections
- Feed forward network
- linear layer
- highway newtork
