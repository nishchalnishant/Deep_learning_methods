## Image models

- The multi layered, hierarchical structure of deep CNN, gives it the ability to extract low, mid, and high-level features.
- Image Models are methods that build representations of images for downstream tasks such as classification and object detection.
- These are the most popular subcategory are convolutional neural networks

- In each of the following models it has several components

**Convulational layers**

> M N*N conv layers which multiplies the matrix of image with itself thus creating a final matrix of p*q*m ( if image was p*q)

> For a coloured image we use a*b*c conv layer thus we get a stack of 3d matrices joined together

**Pooling layers**

> It averages a subarray can be max pooling or min pooling

**Activations functions**

> Lets and neuron pass if it passes threshold

**Batch normalization**

> Normalizes the data on which training is happening to get good result.

**Dropout**

> Reduces overfitting

**Fully connected layers**

> Used to get final result which are connected to loss function to get the final classification result.

## Important image models

**Le-net -- 1998**

- Started using convnet

- LeNet was the first CNN architecture, which not only reduced the number of parameters but was able to learn features from raw pixels automatically.

**Alex net - 2012**

- AlexNet architecture consists of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer.

- Each convolutional layer consists of convolutional filters and a nonlinear activation function ReLU.

- Max pooling layers are used.

- Input size is fixed due to the presence of fully connected layers.

- 60 million parameters.

**Vgg -- 2014**

- VGG16 has a total of 16 layers that has some weights.

- Only Convolution and pooling layers are used.

- Always uses a 3 x 3 Kernel for convolution. 20

- 2×2 size of the max pool.

- 138 million parameters.

- Trained on ImageNet data.

- It has an accuracy of 92.7%.

- Another version that is VGG 19, has a total of 19 layers with weights.

- It is a very good Deep learning architecture for benchmarking on any particular task.

- The pre-trained networks for VGG is made open-source, so it can be commonly used out of the box for various types of applications.

**Google-net -- 2014**

- GoogleNet Architecture is 22 layers deep, with 27 pooling layers included.

- There are 9 inception modules stacked linearly in total.

- The ends of the inception modules are connected to the global average pooling layer.

**Resnet -- 2015**

- ResNet uses a technique called "residual mapping" to combat this issue.

- Instead of hoping that every few stacked layers directly fit a desired underlying mapping, the Residual Network explicitly lets these layers fit a residual mapping.

**Inception v3 -- 2015**

- focuses on burning less computational power by modifying the previous Inception architectures.

- computationally efficient, both in terms of the number of parameters generated by the network and the economical cost incurred

**Dense net -- 2016**

- DenseNet is quite similar to ResNet with some fundamental differences. ResNet uses an additive method (+) that merges the previous layer (identity) with the future layer, whereas DenseNet concatenates the output of the previous layer with the future layer.

- developed specifically to improve the declined accuracy caused by the vanishing gradient in high-level neural networks.

- The DenseNet has different versions, like DenseNet-121, DenseNet-160, DenseNet-201, etc. The numbers denote the number of layers in the neural network. The number 121 is computed as follows:
  5+2*(6+12+24+16)=121
  5 - convolution and pooling layer
  3 - Transition layer(6,12,24)
  1 - Classification layer
  2 - dense block(1*1 and 3\*3 conv)

https://www.pluralsight.com/guides/introduction-to-densenet-with-tensorflow

**Mobile net v1,v2,v3 -- 2018**

- MobileNet is a convolutional neural network architecture that seeks to perform well on mobile devices.
- It is based on an inverted residual structure where the residual connections are between the bottleneck layers.
- The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity.
- As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers.

**Dark net -53 -- 2018**

- Darknet-53 is a convolutional neural network that acts as a backbone for the YOLOv3 object detection approach.
- The improvements upon its predecessor Darknet-19 include the use of residual connections, as well as more layers

**Efficient net v0-v7 --2018**

https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142

https://analyticsindiamag.com/implementing-efficientnet-a-powerful-convolutional-neural-network/

- EfficientNet model was proposed by Mingxing Tan and Quoc V. Le of Google Research, Brain team in their research paper ‘EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks’.

- EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient.

![Development of image models](https://github.com/nishchalnishant/Deep_learning_methods/blob/main/img/Screenshot%202021-09-15%20134934.png)
