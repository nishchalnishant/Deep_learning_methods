> Image models

The multi layered, hierarchical structure of deep CNN, gives it the ability to extract low, mid, and
high-level features.

In each of the following models it has several components --

Convulational layers

- M N*N conv layers which multiplies the matrix of image with itself thus creating a final matrix of p*q*m ( if image was p*q)

- For a coloured image we use a*b*c conv layer thus we get a stack of 3d matrices joined together

Pooling layers

- It averages a subarray can be max pooling or min pooling

Activations functions

- Lets and neuron pass if it passes threshold

Batch normalization

- Normalizes the data on which training is happening to get good result

Dropout

- Reduces overfitting

Fully connected layers

- Used to get final result which are connected to loss function to get the final classification result.

- Le-net -- 1998

- - Started using convnet

- - LeNet was the first CNN architecture, which not only reduced the number of parameters but was able to learn features from raw pixels automatically.

- Alex net - 2012

- - AlexNet architecture consists of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer.

- - Each convolutional layer consists of convolutional filters and a nonlinear activation function ReLU.

- - Max pooling layers are used.

- - Input size is fixed due to the presence of fully connected layers.

- - 60 million parameters.

- Vgg -- 2014

- - VGG16 has a total of 16 layers that has some weights.

- - Only Convolution and pooling layers are used.

- - Always uses a 3 x 3 Kernel for convolution. 20

- - 2Ã—2 size of the max pool.

- - 138 million parameters.

- - Trained on ImageNet data.

- - It has an accuracy of 92.7%.

- - Another version that is VGG 19, has a total of 19 layers with weights.

- - It is a very good Deep learning architecture for benchmarking on any particular task.

- - The pre-trained networks for VGG is made open-source, so it can be commonly used out of the box for various types of applications.

- Google-net -- 2014

- - GoogleNet Architecture is 22 layers deep, with 27 pooling layers included.

- - There are 9 inception modules stacked linearly in total.

- - The ends of the inception modules are connected to the global average pooling layer.

- Resnet -- 2015

- - ResNet uses a technique called "residual mapping" to combat this issue.

- - Instead of hoping that every few stacked layers directly fit a desired underlying mapping, the Residual Network explicitly lets these layers fit a residual mapping.

- Inception v3 -- 2015

- - focuses on burning less computational power by modifying the previous Inception architectures.

- - computationally efficient, both in terms of the number of parameters generated by the network and the economical cost incurred

- Dense net -- 2016
- Mobile net v1,v2,v3 -- 2018
- Dark net -53 -- 2018
- Efficient net v0-v7 --2018

![Development of image models](https://github.com/nishchalnishant/Deep_learning_methods/blob/main/img/Screenshot%202021-09-15%20134934.png)
