## Attnetion module

- Attention Modules refer to modules that incorporate attention mechanisms. - For example, multi-head attention is a module that incorporates multiple attention heads.
- Below you can find a most used list of attention modules.

**Multi head attention**

- Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel.
- The independent attention outputs are then concatenated and linearly transformed into the expected dimension.
- Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).
- Scaled dot product attention
